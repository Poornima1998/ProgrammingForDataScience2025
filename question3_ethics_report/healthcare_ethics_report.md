Data Ethics: AI Ethics in Healthcare Data

A. Healthcare Data Privacy Challenges

Policies such as the GDPR do not provide adequate frameworks for data privacy. Medical information including genetic data, mental health information, treatment histories, etc. demand extra care when working with them (U.S. Department of Health and Human Services, 2025a). . The Health Insurance Portability and Accountability Act (HIPAA) establishes standards for handling protected health information (PHI) with covered entities (healthcare providers, payers, or insurers) with a statutory responsibility of applying policies, legal, and physical safeguards to data and information systems (Edemekong et al., 2024). The Privacy Rule sets PHI use and disclosure controls more stringently permitting no patient consent, and the Security Rule enforces access controls and encryption to e-PHI (U.S. Department of Health and Human Services, 2025b).
Healthcare privacy laws illustrate the challenges of global AI use. The GDPR establishes the baseline for the EU and considers health data sensitive and requiring explicit permission for its processing (European Union, 2018). These differences in legal healthcare privacy frameworks present a challenge for global AI systems, where data may be moved only across borders under the most stringent framework (DLA Piper, 2024).
Data anonymisation has its own challenges and failures. Techniques of removing all the known identifiers of an individual often fail in the face of re-identification.
The augmentation of research endeavours with the need to protect privacy constitutes a defining paradox. AI's use in expanding the frontiers of knowledge is dependent on the use of characteristically aggregated datasets, albeit too much data masking might render the analysis trivial (Price and Cohen, 2019). A framework that advocates for principles of proportionality is recommended, where the use of data is permissible, provided the governance is adequate, and the benefits justify the harm caused (World Health Organization, 2021). In practice, this means datasets and algorithms based on machine learning that are geographically distributed are hybrid federated models as a means of maintaining a degree of privacy while stimulation of further advances is aided (Rieke et al., 2020).

B. Algorithmic Bias in Medical AI

Reasonably representative samples for scenario analysis might be biased to a certain demographic group, to certain geography, or to some determined socioeconomic stratum (Rajkomar et al., 2018). If the dataset is pulled from dominant geographical regions such as the Global North, training the model with data characteristically from such white provinces will result in the model's under- performance on ethnic minorities (Adamson and Smith, 2018).
AI inequities worsen inequities in health outcomes due to biased outcomes. One case in point is an AI algorithm employed in U.S. health systems for resource allocation which underestimated the needs of Black patients by measuring the costs of healthcare and proxying it for how sick the patients were, overlooking the lack of access and barriers patients faced in the system (Obermeyer et al., 2019). Adamson and Smith (2018) point out that skin cancer detection algorithms which were trained on an image base consisting of only light skin do not detect skin cancer on patients of darker skin tones, raising the chances of death in populations that are already marginalised. AI in devices such as pulse oximeters has also displayed racial bias through pulmonary bias, which overestimates oxygen levels in Black patients, delaying much-needed treatment (Sjoding et al., 2021).
It has become essential to define metrics of fairness, such as group and individual fairness, to ascertain the impact of artificial intelligence on the practice of medicine. Barocas et al. (2019) explain group fairness metrics, such as demographic parity, and equalised odds, where member prediction proportions of various groups are equal, and the true and false positive rates are equated.
Addressing bias includes attending to the data imbalance within the data set before model training through oversampling minority groups or synthetic data generation (Chawla et al., 2002). In- processing strategies, like adversarial debiasing, apply fairness constraints on the training phase by teaching models to 'forget' sensitive attributes (Zhang et al., 2018). The All of Us Research Program demonstrates the benefits of inclusive and diverse data sets by focusing on bias mitigation (Denny et al., 2019).

C. Ethical Decision-Making Framework
Developing an ethical decision-making framework for AI in healthcare is important for complexities such as bias and privacy. A practical checklist should start from the top: assessing project alignment with the fundamental principles of beneficence, non-maleficence, justice, and autonomy (Beauchamp and Childress, 2019). These include: evaluating data sources for inclusivity, algorithmic accountability, informed consent, and ex-ante accountability (Floridi et al., 2018).
Informed consent during the big data era is a major challenge, especially when patients have complacent notions about the role of AI in their healthcare (Vayena et al., 2018). Most frameworks should also initiate proactive communication without being prompted, for example, data use and its potential risks with the ability to opt-out (Cohen et al., 2020). The GDPR offers the "right to explanation" which holds AI outputs, in principle, interpretable as per Article 22, enabling patients to dispute automated decisions (European Union, 2018). LIME (Local Interpretable Model- agnostic Explanations) is one of the tools to achieve this (Ribeiro et al., 2016).
Predictive healthcare models can usher in ethical challenges such as becoming overly and incorrectly reliant, and diagnostic and resource expense. UNESCO's Recommendation on the Ethics of AI emphasises observation and sustainability as the core elements (UNESCO, 2021).
    More ethical goals have been aligned to practical components:
    Problem Definition: Ethical goals alignment
    Data Collection: Diversity of data
    Multi-Model Fairness Assessment: Implementation of fairness at every step
    Drift Detection: Post-Deployment Monitoring for Model Drift
    Evaluation: Stakeholder participation in audits.

D. Stakeholder Impact Analysis
Implementing AI in the healthcare industry requires a balance between the various beneficiaries of the applications. Patients stand to gain through improved diagnostics; however, the advent of AI could result in trust erosion due to privacy breaches and biased outcomes (Topol, 2019).
Healthcare practitioners stand to benefit due to increased efficiency, particularly in the analysis of imaging data; however, the consequent loss of autonomy and job displacement remains a concern (Davenport and Kalakota, 2019).
The role of data scientists in responsible AI is to reduce AI bias and ensure compliance. Predictive analytics employed in low-resource settings impact the economy through cost-saving mechanisms; however, the initial expenditure tends to widen the existing gaps. The potential of AI to impact negatively on vulnerable and marginalised communities is a concern, particularly when there is a lack of inclusivity in AI development and deployment (Obermeyer et al., 2019).
The inequitable gaps in the infrastructure development of low- and middle-income countries in relation to AI exacerbate global health inequities. Possible approaches include open-source instruments and global partnerships, such as the WHO's guidelines on equity-centred interventions (World Health Organization, 2021). A stakeholder-balanced approach to the co-design of AI must ensure inclusive shared value distribution.